# Content Aware Recommending Systems with Graph Convolutional Embeddings (CARS-GCE)
#### Double Degree in Mathematics and Computer Science, Universitat de Barcelona
#### Degree's Final Project (TFG), 2020-2021
_____

This project consists in a **Recommender System** that will allow us to compare the performance of different models (**Matrix Factorization** and **Factorization Machines**) set in their optimal hyperparamaters (using **HyperOpt** in order to find them), with the added possibility of applying extensions to said models, such as the use of **Graph Convolutional Embeddings** and online information to make our system more **context-aware**.

You'll find two main folders in this repository's root: code (which contains the whole project) and data (where datasets should be included). We'll now explain how to execute all of the project's workflows.

## **Get your datasets ready**

### **Download your datasets (required)**

_____

First of all, clone the repository locally.

Then, **download at least one of the datasets** available for training the model. You can find them in the following links and **you should include them in the data folder**:

- MovieLens 100K Dataset (ml-100k): https://grouplens.org/datasets/movielens/100k/
- MovieLens 1M Dataset (ml-1m): https://grouplens.org/datasets/movielens/1m/

There's more information on the needed folder structure in `datasources.txt`, file that can be found in the data folder.

### **Extend your datasets (optional)**

_____

In case you are willing to **use online data to extend the previous datasets**, you should **execute the following command in code folder**:

	python dataset_extender.py --api_key=<api_key>

`<api_key>` should be substituted by a **MovieDB API Key** (more information here: https://developers.themoviedb.org/3/getting-started/introduction). 

This command will download information about movie **genres and actors** for all the movies in ml-100k dataset. **We highly recommend execute with hours of anticipation**, since depending on factors such as your internet connection and the dataset you are extending, it may take **several hours to complete and it should not be interrupted.**

Should you interrupt the process, don't you worry: the script will be much faster while processing movies which data has already been downloaded from the API.

**Available arguments:**

`--dataset=ml-1m`

Will switch the dataset you are extending. Example:

	python code/dataset_extender.py --dataset=ml-1m

`--no_genres`

Skips downloading movie genre information while extending the dataset. Examples:

	python code/dataset_extender.py --no_genres
	python code/dataset_extender.py --dataset=ml-1m --no_genres

`--no_actors`

Works the same way as the previous command, but it's used in order to skip downloading information about the actors taking part in the movies.

`--no_post_processing`

Prevents the extender from ignoring actors that are not relevant enough in the dataset to be included in the dataset extension.

`--min_actor_appearances=<value>`

Where `<value>` is an integer, number of movies the actor should take part in in the dataset in order to be considered relevant enough to be included in the extension. Default value is 10.


## **Tune your model settings (optional)**
_____

## **Main exectuion**
_____

**Arguments (optional)**

You can always run this command in order to get the most updated help on arguments:

```
$ python main.py --help
```

--dataset: string. Lets you choose the dataset you want to train the model on.
  Default value: movielens100k
  Available values: movielens100k
  
--add_context: boolean. You can use a context aware system by setting it to True.
  Default value: False
  
--gcn: boolean. Lets you deactivate the Graph Convolutional Network in the Factorization Machine.
  Default value: True
  
--epochs: int. Number of epochs the model will be trained for.
  Default value: 100

--top_k: int. Number of top k recommendations to return.
  Default value: 10

--neg_sample_ratio: int. Number of negative samples generated for every positive sample in the dataset.
  Default value: 4
 
--neg_sample_ratio_test: int. Number of negative samples included in every test set.
  Default value: 99
 
--reduction: 

--batch_size: int. Size of batches generated by our Data Loader
  Default value: 256
  
--device: string. If possible, should be cuda. CPU will always be used if CUDA is not available.
  Default value: cuda if available, cpu if not.
  Available values: cpu, cuda
