# Content Aware Recommending Systems with Graph Convolutional Embeddings (CARS-GCE)
#### Double Degree in Mathematics and Computer Science, Universitat de Barcelona
#### Degree's Final Project (TFG), 2020-2021
_____

This project consists in a **Recommender System** that will allow us to compare the performance of different models (**Matrix Factorization** and **Factorization Machines**) set in their optimal hyperparamaters (using **HyperOpt** in order to find them), with the added possibility of applying extensions to said models, such as the use of **Graph Convolutional Embeddings** and online information to make our system more **context aware**.

You'll find two main folders in this repository's root: `code` (which contains the whole project) and `data` (where datasets should be included). We'll now explain how to execute all of the project's workflows.

## **Get your datasets ready**

### **Download your datasets (required)**

_____

First of all, clone the repository locally.

Then, **download at least one of the datasets** available for model training. We can do this by **executing the following command in `code` folder**:

	python dataset_downloader.py

This script will download the **MovieLens 100k dataset**. In case you want to download the **MovieLens 1M dataset**, you may use the `dataset` flag this way:

	python dataset_downloader.py --dataset=ml-1m

### **Extend your datasets (optional)**

_____

In case you are willing to **use online data to extend the previous datasets**, you should **execute the following command in `code` folder**:

	python dataset_extender.py --api_key=<api_key>

`<api_key>` should be substituted by a **MovieDB API Key** owned by you (more information here: https://developers.themoviedb.org/3/getting-started/introduction). 

This command will download information about movie **genres and actors** for all the movies in ml-100k dataset. **We highly recommend executing this script with hours of anticipation**, since depending on factors such as your internet connection and the dataset you are extending, it may take **several hours to complete and it should not be interrupted.**

Should you interrupt the process, don't you worry: the script will be much faster while processing movies which data has already been downloaded from the API.

**Available arguments:**

`--dataset=ml-1m`

Will switch the dataset you are extending. Example:

	python dataset_extender.py --dataset=ml-1m

`--no_genres`

Skips downloading movie genre information while extending the dataset. Examples:

	python dataset_extender.py --no_genres
	python dataset_extender.py --dataset=ml-1m --no_genres

`--no_actors`

Works the same way as the previous command, but it's used in order to skip downloading information about the actors taking part in the movies.


### **Post-process your datasets (optional)**

_____

We include a dataset to **filter out actors with few appearances** along the movies included in the datasets. **This script shouldn't be executed unless you extended your datasets first**. Execute the following command in the `code` folder to post-process the MovieLens-100k dataset: 

	python dataset_postprocessor.py

`--dataset=ml-1m`

Will switch the dataset you are post-processing. Example:

	python dataset_postprocessor.py --dataset=ml-1m

`--min_actor_appearances=<value>`

Where `<value>` is an integer, number of movies the actor should take part in in the dataset in order to be considered relevant enough to be included in the extension. Default value is 10.


## **Tune your model settings (optional)**
_____

## **Main exectuion**
_____

**Arguments (optional)**

You can always run this command in order to get the most updated help on arguments:

```
$ python main.py --help
```

--dataset: string. Lets you choose the dataset you want to train the model on.
  Default value: movielens100k
  Available values: movielens100k
  
--add_context: boolean. You can use a context aware system by setting it to True.
  Default value: False
  
--gcn: boolean. Lets you deactivate the Graph Convolutional Network in the Factorization Machine.
  Default value: True
  
--epochs: int. Number of epochs the model will be trained for.
  Default value: 100

--top_k: int. Number of top k recommendations to return.
  Default value: 10

--neg_sample_ratio: int. Number of negative samples generated for every positive sample in the dataset.
  Default value: 4
 
--neg_sample_ratio_test: int. Number of negative samples included in every test set.
  Default value: 99
 
--reduction: 

--batch_size: int. Size of batches generated by our Data Loader
  Default value: 256
  
--device: string. If possible, should be cuda. CPU will always be used if CUDA is not available.
  Default value: cuda if available, cpu if not.
  Available values: cpu, cuda
